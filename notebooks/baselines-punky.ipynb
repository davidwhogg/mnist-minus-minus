{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SOVH_6gOtPM"
   },
   "source": [
    "# MNIST-minus-minus: Train and test baselines for punky datasets\n",
    "\n",
    "## Authors\n",
    "- **David W Hogg** (NYU) (Flatiron)\n",
    "- **Soledad Villar** (JHU)\n",
    "\n",
    "## To-Do / Bugs:\n",
    "\n",
    "## Notes\n",
    "- We got some of the sample code from the (excellent) *jax* examples documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe_qAE4zOq9D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from jax.scipy.special import logsumexp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "import os\n",
    "import ssl\n",
    "from urllib.request import urlopen\n",
    "from shutil import copyfileobj\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all MNIST++ datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "baseurl = \"https://cosmo.nyu.edu/hogg/research/2023/04/17/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_read_pickle(filename, clobber=False):\n",
    "    if clobber or not os.path.isfile(filename):\n",
    "        with urlopen(baseurl + filename) as in_stream, open(filename, 'wb') as out_file:\n",
    "            copyfileobj(in_stream, out_file)\n",
    "    with gzip.open(filename, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SixtyNine++\n",
    "(X_train69, M_train69, y_train69), (X_test69, M_test69, y_test69) = get_and_read_pickle(\"SixtyNine++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LowRes++\n",
    "(X_trainLow, M_trainLow, y_trainLow), (X_testLow, M_testLow, y_testLow) = get_and_read_pickle(\"LowRes++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CutOut++\n",
    "(X_trainCut, M_trainCut, y_trainCut), (X_testCut, M_testCut, y_testCut) = get_and_read_pickle(\"CutOut++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CutOut++\n",
    "(X_trainProj, M_trainProj, y_trainProj), (X_testProj, M_testProj, y_testProj) = get_and_read_pickle(\"Projections++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Crops++\n",
    "(X_trainCrop, M_trainCrop, y_trainCrop), (X_testCrop, M_testCrop, y_testCrop) = get_and_read_pickle(\"Crops++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Jitter++\n",
    "(X_trainJit, M_trainJit, y_trainJit), (X_testJit, M_testJit, y_testJit) = get_and_read_pickle(\"Jitter++.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up MLP model\n",
    "*Note:* Most of this code is copied from the *jax* documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = jnp.dot(final_w, activations) + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(X, label_list, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding\"\"\"\n",
    "    foo = jnp.array([x == label_list for x in X], dtype)\n",
    "    while len(foo.shape) > 2:\n",
    "        foo = foo.all(axis=-1)\n",
    "    return foo\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y, step_size):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [(w - step_size * dw, b - step_size * db)\n",
    "              for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "def train_and_test(X_train, y_train, X_test, y_test, epochs=8):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_test) == len(y_test)\n",
    "\n",
    "    num_pixels = X_train[0].shape[0] * X_train[0].shape[1]\n",
    "    label_list = np.unique(y_train, axis=0)\n",
    "    num_labels = len(label_list)\n",
    "    layer_sizes = [num_pixels, 512, 512, num_labels] # MAGIC\n",
    "    step_size = 0.01 # MAGIC\n",
    "    batch_size = 128 # MAGIC\n",
    "    n_targets = num_labels\n",
    "    print(\"Found {} distinct labels, and input images with {} pixels\".format(num_labels, num_pixels))\n",
    "    print(\"The labels are {}\".format(label_list))\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_images = jnp.reshape(X_train, (len(X_train), num_pixels))\n",
    "    train_labels = one_hot(y_train, label_list)\n",
    "    test_images = jnp.reshape(X_test, (len(X_test), num_pixels))\n",
    "    test_labels = one_hot(y_test, label_list)\n",
    "    print(\"Now train_labels is {}\".format(train_labels.shape))\n",
    "    print(\"Their sums are {}\".format(np.sum(train_labels, axis=0)))\n",
    "\n",
    "    params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        for x, y in tfds.as_numpy(train_dataset.batch(batch_size).prefetch(1)):\n",
    "            x = jnp.reshape(x, (len(x), num_pixels))\n",
    "            y = one_hot(y, label_list)\n",
    "            params = update(params, x, y, step_size)\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        train_acc = accuracy(params, train_images, train_labels)\n",
    "        test_acc = accuracy(params, test_images, test_labels)\n",
    "        print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "        print(\"Training set accuracy {}\".format(train_acc))\n",
    "        print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test MLP model on the 5 easy cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge nobody asked for\n",
    "train_and_test(X_trainCrop, y_trainCrop, X_testCrop, y_testCrop, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge nobody asked for\n",
    "train_and_test(X_trainCrop, y_trainCrop, X_testCrop, y_testCrop, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge nobody asked for\n",
    "train_and_test(X_trainProj, y_trainProj, X_testProj, y_testProj, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge nobody asked for\n",
    "train_and_test(X_trainCut, y_trainCut, X_testCut, y_testCut, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge nobody asked for\n",
    "train_and_test(X_trainLow, y_trainLow, X_testLow, y_testLow, epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIYrE61nDcLVCM+1Hb2Xuq",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
