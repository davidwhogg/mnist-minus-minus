{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SOVH_6gOtPM"
   },
   "source": [
    "# Baselines\n",
    "\n",
    "A handwritten-digit reading task, now with more chaos!\n",
    "\n",
    "## Authors\n",
    "- **David W Hogg** (NYU) (Flatiron)\n",
    "- **Soledad Villar** (JHU)\n",
    "\n",
    "## To-Do / Bugs:\n",
    "- Need to keep and report group-element labels, not just content labels.\n",
    "- Need to define the group-element and geometric transform basis / transpose. Need to test correctness.\n",
    "- Need to package output or data sets with pip or zenodo or somesuch.\n",
    "\n",
    "## Notes\n",
    "- Some content copied from <https://github.com/wxs/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb>.\n",
    "- Some content copied from <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe_qAE4zOq9D"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "rng = np.random.default_rng(17) # The most random of all possible seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Fashion++\n",
    "with gzip.open('Fashion++.pkl.gz', 'rb') as file:\n",
    "    (X_trainf, M_trainf, y_trainf), (X_testf, M_testf, y_testf) = pickle.load(file)\n",
    "print(X_trainf.shape, M_trainf.shape, y_trainf.shape,\n",
    "      X_testf.shape,  M_testf.shape,  y_testf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MNIST+4\n",
    "with gzip.open('MNIST+4.pkl.gz', 'rb') as file:\n",
    "    (X_train4, M_train4, y_train4), (X_test4, M_test4, y_test4) = pickle.load(file)\n",
    "print(X_train4.shape, M_train4.shape, y_train4.shape,\n",
    "      X_test4.shape,  M_test4.shape,  y_test4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at label statistics for MNIST+4\n",
    "sstr = set(y_train4)\n",
    "print(\"total number of labels missing from the training set:\", 10000 - len(sstr))\n",
    "sste = set(y_test4)\n",
    "print(\"total number of labels missing from the test set:\", 10000 - len(sste))\n",
    "i = 0\n",
    "for q in sste:\n",
    "    if q not in sstr:\n",
    "        i += 1\n",
    "        print(i, \"label\", q, \"is in the test set but not in the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfy5ygvrsBNt"
   },
   "outputs": [],
   "source": [
    "# Read MNIST+9\n",
    "with gzip.open('MNIST+9.pkl.gz', 'rb') as file:\n",
    "    (X_train9, M_train9, y_train9), (X_test9, M_test9, y_test9) = pickle.load(file)\n",
    "print(X_train9.shape, M_train9.shape, y_train9.shape,\n",
    "      X_test9.shape,  M_test9.shape,  y_test9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yl_dvhGMYMko"
   },
   "outputs": [],
   "source": [
    "# Read MNIST+Inf\n",
    "with gzip.open('MNIST+Inf.pkl.gz', 'rb') as file:\n",
    "    (X_trainInf, M_trainInf, y_trainInf), (X_testInf, M_testInf, y_testInf) = pickle.load(file)\n",
    "print(X_trainInf.shape, M_trainInf.shape, y_trainInf.shape,\n",
    "      X_testInf.shape,  M_testInf.shape,  y_testInf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW PACK THE ABOVE INTO tensorflow dataset objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = jnp.dot(final_w, activations) + final_b\n",
    "  return logits - logsumexp(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  preds = batched_predict(params, images)\n",
    "  return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "num_pixels = X_trainf[0].shape[0] * X_trainf[0].shape[1]\n",
    "num_labels = len(set(y_trainf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's upgrade it to handle batches using `vmap`\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_trainf, y_trainf))\n",
    "\n",
    "train_images = jnp.reshape(X_trainf, (len(X_trainf), num_pixels))\n",
    "train_labels = one_hot(y_trainf, num_labels)\n",
    "\n",
    "test_images = jnp.reshape(X_testf, (len(X_testf), num_pixels))\n",
    "test_labels = one_hot(y_testf, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in tfds.as_numpy(train_dataset.batch(batch_size).prefetch(1)):\n",
    "    x = jnp.reshape(x, (len(x), num_pixels))\n",
    "    y = one_hot(y, num_labels)\n",
    "    params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIYrE61nDcLVCM+1Hb2Xuq",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
